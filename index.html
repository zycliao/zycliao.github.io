
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <title>Zhouyingcheng Liao</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Rubik:300,500,700" rel="stylesheet">
    <link href="./assets/web_icon.png" rel="icon" type="image/png">
    <style>
        body {
            font-family:'Rubik',-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Oxygen,Ubuntu,Cantarell,'Open Sans','Helvetica Neue',sans-serif;
            font-size:calc(10px+0.33vw);
            -webkit-font-smoothing:antialiased;
            padding:5vh 20vw;
            color:#121314;
        }
        p {
            font-weight:300;
        }
        b {
            font-weight:bolder;
        }
        a {
            color:#1367a7;
            text-decoration:none;
        }
        a:hover:after {
            top:0;
        }
        .table-img {
            width:30%;
            padding-top:20px;
            float:left;
        }
        .table-content {
            width:70%;
            float:left;
            padding-bottom:20px;
        }
        .paper-img {
            width:30%;
            float:left;
        }
        .icon {
            width: 1.8rem;
            height: 1.8rem;
            margin-right: 0.8rem;
        }
        @media screen and (max-device-width:700px) {
            body {
                padding:5vh 5vw;
            }
            .table-img {
                width:30%;
                padding-top:30px;
            }
            .table-content {
                width:70%;
            }
        }
    </style>
</head>

<body >
    <div style="display: flex; justify-content: row; width: 100%;">
        <div class="table-img">
            <img src="./assets/my_photo.jpg" width="75%" style="max-width: 200px;" alt="Zhouyingcheng Liao">
        </div>
        <div class="table-content" style="font-size:1em; line-height: 1">
            <p style="font-weight:500; font-size:1.8em">Zhouyingcheng Liao (ÂªñÂë®Â∫îÊàê)<br></p>
            <p>Ph.D Student </p>
            <p>The University of Hong Kong (HKU)</p>
            <p style="margin-top: 1em"><strong>Email</strong>: zycliao@cs.hku.hk or zycliao@gmail.com</p>
            <p>
                <a href="https://scholar.google.com/citations?user=ldtL6LwAAAAJ&hl=en" target="_blank"><img src="./assets/icons/google_scholar.svg" alt="GoogleScholar" class="icon"></a>
                <a href="https://www.linkedin.com/in/zhouyingcheng-liao-a7525814a/" target="_blank"><img src="./assets/icons/linkedin.svg" alt="LinkedIn" class="icon"></a>
                <a href="https://github.com/zycliao" target="_blank"><img src="./assets/icons/github.svg" alt="Github" class="icon"></a>
                <a href="https://www.instagram.com/zycliao/" target="_blank"><img src="./assets/icons/instagram.svg" alt="Instagram" class="icon"></a>
            </p>
        </div>
    </div>

    <div style="font-size:1.15em; line-height: 1.4; margin-bottom: 50px;">
        <p>üòÑ I am a Ph.D. student <sup style="font-size:0.8em;">since Jan. 2023</sup> at the University of Hong Kong, supervised by <a href="https://hku-cg.github.io/" target="_blank">Taku Komura</a>. I obtained my M.Sc. degree at Saarland University and Max Planck Institute for Informatics (supervisor: <a href="https://gvdh.mpi-inf.mpg.de/" target="_blank"> Marc Habermann</a>) and B.Sc. degree at Shanghai Jiao Tong University.</p>
        
        <p>I had a few wonderful research internship experiences at different companies and institutes, namely Adobe Research (2021, mentor: <a href="https://yzhou359.github.io/" target="_blank"> Yang Zhou</a>), miHoYo (2020, mentor: <a href="https://junxnui.github.io/" target="_blank"> Jun Xing</a>), MPI-INF (2019, supervisor: <a href="https://virtualhumans.mpi-inf.mpg.de" target="_blank"> Gerard Pons-Moll.</a>) </p>

        <p>My research interests lie in the intersection of computer vision and computer graphics. More specifically, I am interested in modeling and animating digital characters (including the body and the clothes) driven by data. My research goal is to develop algorithms that can free content creators from hard labours.</p>
    </div>
    
<!--     <div style="margin-bottom: 50px;">
        <h1 id="prospective">üôåFor Prospective Student</h1>
        <div style="font-size:1.15em; line-height: 1.4">
        <p>Our team is seeking undergraduate/master students interested in human pose estimation, motion control, and Large-Language-Motion-Models (LLMM).
            We offer opportunities to work on your thesis or join as a Research Assistant. </p>
        <p>üëá What you can get: <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ Cutting-edge research in human-compute graphics. <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ Hands-on experience with high-end equipment: advanced mesh scanners and motion capture system. <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ Collaborative thesis development or full-time research role. <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚úÖ Recommendation letters for your applications. <br>
            If you feel interested, just send me an email (zycliao@cs.hku.hk). </p>
        </div>
    </div> -->

    <div style="margin-bottom: 50px;">
        <h1 id="publications">üìö Publications</h1>
		
		<div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/senc.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">SENC: Handling Self-collision in Neural Cloth Simulation</b>
                <p><b>Zhouyingcheng Liao*</b>, Sinan Wang, Taku Komura</p>
                <p><b>ECCV 2024</b></p>
				<p> [<a href="" target="_blank">Paper and code coming soon!</a>]
                    
                    </p>
				
                <p>A self-supervised neural cloth simulator that effectively addresses cloth self-collision.</p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/EMDM.gif" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation</b>
                <p>Wenyang Zhou, Zhiyang Dou, Zeyu Cao, <b>Zhouyingcheng Liao</b>, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, Lingjie Liu</p>
                <p><b>ECCV 2024</b></p>
                <p>[<a href="https://frank-zy-dou.github.io/projects/EMDM/index.html" target="_blank">webpage</a>]
                    [<a href="https://arxiv.org/abs/2312.02256" target="_blank">paper</a>]
                    [<a href="https://youtu.be/1SyCXbnol_g?si=atUuUP4eLbXQjixc" target="_blank">video</a>]
                    </p>
                <p>A fast and high-quality human motion generation method, which takes only 0.05s for a sequence of 196 frames.</p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/VINECS.gif" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">VINECS: Video-based Neural Character Skinning</b>
                <p><b>Zhouyingcheng Liao</b>, Vladislav Golyanik, Marc Habermann, Christian Theobalt</p>
                <p><b>CVPR 2024</b></p>
                <p>[<a href="https://arxiv.org/abs/2307.00842" target="_blank">paper</a>]
                    </p>
                <p>The first end-to-end method for generating a dense and rigged 3D character mesh with learned pose-dependent skinning weights solely from multi-view videos.</p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/sftp.gif" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Skeleton-free Pose Transfer for Stylized 3D Characters</b>
                <p><b>Zhouyingcheng Liao</b>, Jimei Yang, Jun Saito, Gerard Pons-Moll, Yang Zhou</p>
                <p><b>ECCV 2022</b></p>
                <p>[<a href="https://zycliao.com/sfpt/" target="_blank">webpage</a>]
                    [<a href="https://arxiv.org/abs/2208.00790" target="_blank">paper</a>]
                    [<a href="https://github.com/zycliao/skeleton-free-pose-transfer" target="_blank">code</a>]
                    </p>
                <p>The first neural method that achieves automatic pose transfer between any stylized 3D characters, without any rigging, skinning or manual correspondence.</p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/tailornet.gif" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">TailorNet: Predicting Clothing in 3d as a Function of Human Pose, Shape and Garment Style</b>
                <p>Chaitanya Patel*, <b>Zhouyingcheng Liao*</b>, Gerard Pons-Moll (*: co-first author)</p>
                <p><b>CVPR 2020 <span style="color:red">oral</span></b></p>
                <p>[<a href="https://virtualhumans.mpi-inf.mpg.de/tailornet/" target="_blank">webpage</a>]
                    [<a href="https://arxiv.org/abs/2003.04583" target="_blank">paper</a>]
                    [<a href="https://youtu.be/F0O21a_fsBQ" target="_blank">video</a>]
                    [<a href="https://github.com/chaitanya100100/TailorNet" target="_blank">code</a>]
                    [<a href="https://github.com/zycliao/TailorNet_dataset" target="_blank">data</a>]
                    </p>
                <p>TailorNet is the first neural model which predicts clothing deformation in 3D as a function of three factors: pose, shape and style (garment geometry), while retaining wrinkle detail. We also present a garment animation dataset of 55800 frames generated by physically based simulation </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/liveface.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Live Face Verification with Multiple Instantialized Local Homographic Parameterization.</b>
                <p>Chen Lin, <b>Zhouyingcheng Liao</b>, Peng Zhou, Jianguo Hu, Bingbing Ni</p>
                <p><b>IJCAI 2018</b></p>
                <p>[<a href="https://www.ijcai.org/proceedings/2018/0113.pdf" target="_blank">paper</a>]</p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publication_teaser/uniface.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Uniface: A Unified Network for Face Detection and Recognition</b>
                <p><b>Zhouyingcheng Liao</b>, Peng Zhou, Qinlong Wu, Bingbing Ni</p>
                <p><b>ICPR 2018</b></p>
                <p>[<a href="./assets/uniface.pdf" target="_blank">paper</a>]</p>
            </div>
            <div style="clear: both;"></div>
        </div>

    </div>


    <div>
        <h1>üéì Academic Service</h1>
        <div style="font-size:1.15em;">
            <h3>Reviewer</h3>
			<p>Siggraph Asia 2023, 2024</p>
            <p>Eurographics 2024</p>
            <p>CVPR 2023, 2024</p>
            <p>TVCG</p>
            <p>Computer & Graphics</p>
            <h3>Teaching Assistant</h3>
            <p>Data-driven Computer Animation (COMP3360/7508@HKU) Spring 2024</p>
            <p>Computer Game Design and Programming (COMP3329@HKU) Spring 2024</p>
            <p>Computer Game Design and Programming (COMP3329@HKU) Spring 2023</p>
        </div>
    </div>
</body>

</html>
